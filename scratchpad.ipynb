{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 11:24:33.805733: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-05 11:24:34.719410: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-05 11:24:34.719533: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-05 11:24:34.719550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from neurallogic import neural_logic_net, harden, harden_layer, hard_or, hard_and, hard_not, primitives, symbolic_primitives\n",
    "from tests import test_mnist\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurallogic import sym_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.interpreters import xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the GPU memory\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandpit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_jaxpr(closed_jaxpr):\n",
    "   jaxpr = closed_jaxpr.jaxpr\n",
    "   print(\"invars:\", jaxpr.invars)\n",
    "   print(\"outvars:\", jaxpr.outvars)\n",
    "   print(\"constvars:\", jaxpr.constvars)\n",
    "   for eqn in jaxpr.eqns:\n",
    "     print(\"equation:\", eqn.invars, eqn.primitive, eqn.outvars, eqn.params)\n",
    "   print()\n",
    "   print(\"jaxpr:\", jaxpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = test_mnist.get_datasets()\n",
    "train_ds[\"image\"] = jnp.reshape(train_ds[\"image\"], (train_ds[\"image\"].shape[0], -1))\n",
    "test_ds[\"image\"] = jnp.reshape(test_ds[\"image\"], (test_ds[\"image\"].shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nln(type, x, width):\n",
    "    x = hard_or.or_layer(type)(width, nn.initializers.uniform(1.0), dtype=jnp.float32)(x) \n",
    "    #x = hard_not.not_layer(type)(10, dtype=jnp.float32)(x)\n",
    "    #x = primitives.nl_ravel(type)(x) \n",
    "    #x = harden_layer.harden_layer(type)(x) \n",
    "    #x = primitives.nl_reshape(type)((10, width))(x) \n",
    "    #x = primitives.nl_sum(type)(-1)(x) \n",
    "    return x\n",
    "\n",
    "def batch_nln(type, x, width):\n",
    "    return jax.vmap(lambda x: nln(type, x, width))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 10\n",
    "soft, hard, _ = neural_logic_net.net(lambda type, x: nln(type, x, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        HardOrLayer_0: {\n",
       "            weights: DeviceArray([[ True, False, False, ...,  True,  True,  True],\n",
       "                         [False, False, False, ...,  True, False, False],\n",
       "                         [False,  True,  True, ...,  True,  True, False],\n",
       "                         ...,\n",
       "                         [False,  True, False, ...,  True, False, False],\n",
       "                         [False,  True, False, ...,  True,  True, False],\n",
       "                         [ True, False,  True, ..., False, False, False]], dtype=bool),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "mock_input = harden.harden(jnp.ones([28 * 28]))\n",
    "hard_weights = harden.hard_weights(soft.init(rng, mock_input))\n",
    "hard_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jaxpr = jax.make_jaxpr(lambda x: hard.apply(hard_weights, x))(harden.harden(test_ds['image'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invars: [b]\n",
      "outvars: [d]\n",
      "constvars: [a]\n",
      "equation: [a, b] xla_call [c] {'device': None, 'backend': None, 'name': 'hard_or_include', 'donated_invars': (False, False), 'inline': False, 'keep_unused': False, 'call_jaxpr': { lambda ; a:bool[10,784] b:bool[784]. let\n",
      "    c:bool[1,784] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 784)] b\n",
      "    d:bool[10,784] = and c a\n",
      "  in (d,) }}\n",
      "equation: [c] reduce_or [d] {'axes': (1,)}\n",
      "\n",
      "jaxpr: { lambda a:bool[10,784]; b:bool[784]. let\n",
      "    c:bool[10,784] = xla_call[\n",
      "      call_jaxpr={ lambda ; d:bool[10,784] e:bool[784]. let\n",
      "          f:bool[1,784] = broadcast_in_dim[\n",
      "            broadcast_dimensions=(1,)\n",
      "            shape=(1, 784)\n",
      "          ] e\n",
      "          g:bool[10,784] = and f d\n",
      "        in (g,) }\n",
      "      name=hard_or_include\n",
      "    ] a b\n",
      "    h:bool[10] = reduce_or[axes=(1,)] c\n",
      "  in (h,) }\n"
     ]
    }
   ],
   "source": [
    "examine_jaxpr(jaxpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: jax primitives and symbolic primitives are identical.\n",
      "SUCCESS: non-standard evaluation is identical to standard evaluation of jaxpr.\n",
      "eval_hard_output [ True  True  True  True  True  True  True  True  True  True]\n",
      "SUCCESS: dimensions of non-standard evaluation and standard evaluation of jaxpr are identical.\n",
      "reduced_eval_symbolic_output: [ True  True  True  True  True  True  True  True  True  True]\n",
      "SUCCESS: values of symbolic evaluation and standard evaluation of jaxpr are identical.\n"
     ]
    }
   ],
   "source": [
    "hard_mock_input = harden.harden(test_ds['image'][0])\n",
    "hard_output = hard.apply(hard_weights, hard_mock_input)\n",
    "#print(\"hard_output shape:\", hard_output.shape)\n",
    "#print(\"hard_output:\", hard_output)\n",
    "eval_hard_output, eval_symbolic_output = sym_gen.eval_jaxpr(False, jaxpr.jaxpr, jaxpr.literals, hard_mock_input)\n",
    "#print(\"eval_hard_output:\", eval_hard_output)\n",
    "#print(\"eval_symbolic_output:\", eval_symbolic_output)\n",
    "assert numpy.array_equal(numpy.array(eval_hard_output), eval_symbolic_output)\n",
    "print(\"SUCCESS: jax primitives and symbolic primitives are identical.\")\n",
    "standard_jax_output = hard.apply(hard_weights, hard_mock_input)\n",
    "#print(\"standard_jax_output\", standard_jax_output)\n",
    "#print(\"eval_hard_output\", numpy.array(eval_hard_output))\n",
    "assert jax.numpy.array_equal(numpy.array(eval_hard_output), standard_jax_output)\n",
    "print(\"SUCCESS: non-standard evaluation is identical to standard evaluation of jaxpr.\")\n",
    "symbolic_mock_input = symbolic_primitives.to_boolean_string(hard_mock_input)\n",
    "#print(\"symbolic_mock_input:\", symbolic_mock_input)\n",
    "#print(\"type of symbolic_mock_input = \", type(symbolic_mock_input))\n",
    "#print(\"type of element = \", symbolic_mock_input.dtype)\n",
    "symbolic_jaxpr_literals = symbolic_primitives.to_boolean_string(jaxpr.literals)\n",
    "#print(\"jaxpr.literals = \", symbolic_jaxpr_literals)\n",
    "#print(\"type of jaxpr.literals = \", type(symbolic_jaxpr_literals))\n",
    "#print(\"type of element = \", symbolic_jaxpr_literals.dtype)\n",
    "eval_symbolic_output = sym_gen.eval_jaxpr(True, jaxpr.jaxpr, symbolic_jaxpr_literals, symbolic_mock_input)\n",
    "# assert the dimensions of eval_hard_output and eval_symbolic_output are the same\n",
    "eval_hard_output = numpy.array(eval_hard_output)\n",
    "print(\"eval_hard_output\", eval_hard_output)\n",
    "#print(\"type of eval_hard_output = \", type(eval_hard_output))\n",
    "#print(\"eval_symbolic_output:\", eval_symbolic_output)\n",
    "#print(\"type of eval_symbolic_output = \", type(eval_symbolic_output))\n",
    "#print(\"shape of eval_hard_output = \", eval_hard_output.shape)\n",
    "#print(\"shape of eval_symbolic_output = \", eval_symbolic_output.shape)\n",
    "assert numpy.array_equal(eval_hard_output.shape, eval_symbolic_output.shape)\n",
    "print(\"SUCCESS: dimensions of non-standard evaluation and standard evaluation of jaxpr are identical.\")\n",
    "# assert the values of eval_hard_output and eval_symbolic_output are the same\n",
    "reduced_eval_symbolic_output = symbolic_primitives.symbolic_eval(eval_symbolic_output)\n",
    "print(\"reduced_eval_symbolic_output:\", reduced_eval_symbolic_output)\n",
    "assert numpy.array_equal(eval_hard_output, reduced_eval_symbolic_output)\n",
    "print(\"SUCCESS: values of symbolic evaluation and standard evaluation of jaxpr are identical.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
