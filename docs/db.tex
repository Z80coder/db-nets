
\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{comment}

\title{$\partial\mathbb{B}$ nets: learning boolean functions by\\gradient descent}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ian Wright}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newtheorem{theorem}{Theorem}
\newtheorem*{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
	$\partial\mathbb{B}$ nets are differentiable neural networks 
	that learn discrete boolean functions by gradient descent.
	$\partial\mathbb{B}$ nets, once trained, `harden' to boolean functions with identical semantics. In consequence, the boolean functions have identical accuracy to the trained nets, unlike existing approaches to neural network binarization. Experiments demonstrate that $\partial\mathbb{B}$ nets achieve comparable performance on standard machine learning problems yet are significantly more compact (due to 1-bit weights) and interpretable (due to the logical nature of the learnt function).
\end{abstract}

\section{Introduction}

Typical neural networks are differentiable functions with weights represented by machine floats. Training consists of gradient descent in weight-space, where the direction of descent minimises loss and the gradient is efficiently calculated by the backpropagation algorithm \citep{rumelhart1986learning}. This approach has led to tremendous advances in machine learning.

However, there are drawbacks. First, the differentiability requirement means we cannot easily learn discrete functions, such as logical predicates. In consequence, interpreting or verifying what a network has learned is difficult. Also, many learning problems naturally arise in discrete rather than continuous domains. Second, representing weights as machine floats enables time-efficient training but at the cost of memory-inefficient models. For example, network quantisation techniques (TODO:refs) demonstrate that full 64 of 32-bit precision weights are often unnecessary for final predictive performance, although there is a trade-off.

This paper proposes a new approach to mitigating these drawbacks. The main idea is to define a type of neural network, called a $\partial \mathbb{B}$ net, which has two aspects: a soft net, which is a differentiable real-valued function, and a hard net, which is a non-differentiable, discrete function. Both aspects are semantically equivalent. We train the soft net as normal, using backpropagation, then `harden' the learned weights to boolean values and bind them with the hard net to yield a discrete function with identical predictive performance (see figure \ref{fig:main-idea}). In consequence, interpreting and verifying a $\partial \mathbb{B}$ net is relatively less difficult. The bias towards learning discrete functions reduces variance in some domains. And boolean-valued, 1-bit weights increase the memory-efficiency of trained models.

The main contributions of this work are (i) defining novel activation functions that `harden' to semantically equivalent boolean functions, (ii) defining novel network architectures to effectively learn boolean functions that solve multi-class classification problems, and (iii) experiments that demonstrate $\partial \mathbb{B}$ nets compete with existing approaches in terms of predictive performance yet yield considerably smaller models.

This paper examines related work (section \ref{sec:related-work}), (ii) etc.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{db-net.png}
	\caption{{\em Learning discrete functions with a $\partial\mathbb{B}$ net.} A $\partial \mathbb{B}$ net specifies (i) a differentiable neural network that is hard-equivalent to (ii) a non-differentiable discrete function. The neural network is trained as normal with backpropagation to yield a set of real weights. The real weights are hardened to boolean values and then bound with the discrete function. The result is a learned discrete function that performs identically to the trained network.}
	\label{fig:main-idea}
\end{figure}


\section{Related work}\label{sec:related-work}

\section{$\partial\mathbb{B}$ nets}

A $\partial \mathbb{B}$ net has two aspects, a soft net and a hard net. Both nets use bits to represent transitory values and learnable weights. However, a soft net uses soft-bits and a hard net uses hard-bits.

\begin{definition}[Soft-bits and hard-bits]
A {\em soft-bit} is a real value in the range $[0,1]$ and a {\em hard-bit} is a boolean value from the set $\{0,1\}$. A soft-bit, $x$, is {\em high} if $x>1/2$, otherwise it is {\em low}.
\end{definition}

A hardening function converts soft-bits to hard-bits.

\begin{definition}[Hardening]
The {\em hardening} function, $\operatorname{harden}(x_{1}, \dots, x_{n}) = [f(x_{1}), \dots, f(x_{n})]$, converts soft-bits to hard-bits, where
\begin{equation*}
f(x) =
\begin{cases}
1 & \text{if } x > 1/2 \\
0 & \text{otherwise.}
\end{cases}
\end{equation*}
\end{definition}

The soft-bit value $1/2$ is therefore a threshold. Above this threshold the soft-bit represents $\text{True}$, otherwise it represents $\text{False}$.

A soft net is any differentiable function, $f$, that `hardens' to a semantically equivalent discrete function, $g$. For example, if $f(x) = 1 - x$, where $x \in [0,1]$, and $g(y) = \neg y$, where $y \in \{0,1\}$ then: if $x$ is high (resp. low) then both $f(x)$ and $g(\operatorname{harden}(x))$ are low (resp. high). In other words, $f$ is hard-equivalent to boolean negation. More generally:

\begin{definition}[Hard-equivalence]
	A function, $f: [0,1]^n \rightarrow [0,1]^m$, is {\em hard-equivalent} to a discrete function, $g: \{1,0\}^n \rightarrow \{1,0\}^m$,	if
	\begin{equation*}
	\operatorname{harden}(f(\operatorname{harden}({\bf x}))) = g(\operatorname{harden}({\bf x}))
	\end{equation*}
for all ${\bf x} \in [0,1]^{n}$. For shorthand write $f \equiv g$.
\end{definition}

Neural networks are typically composed of nonlinear activation functions (for representational generality) that are strictly monotonic (so gradients always exist that link changes in inputs to outputs) and differentiable (so gradients reliably represent the local loss surface). Activation functions that are monotonic but not strictly so (and therefore some gradients are zero) and differentiable almost everywhere (and therefore some gradients are undefined) also work, e.g. RELU \citep{10.5555/3104322.3104425}. $\partial \mathbb{B}$ nets are arbitrary compositions of `activation' functions that also satisfy these properties but in addition are hard-equivalent to boolean functions (and natural generalisations).

\begin{figure}[t]
	\centering
	\includegraphics[trim=0pt 0pt 0pt 0pt, clip, width=1.0\textwidth]{logic-gates.png}
	\caption{{\em Gradient-rich versus gradient-sparse differentiable boolean functions.} Each column contains contour plots of functions $f(x,y)$ that are hard-equivalent to a boolean function (one of $\neg(x \oplus y)$, $x \wedge y$, $x \vee y$, or $x \Rightarrow y$). Every function is continuous and differentiable almost everywhere (white lines indicate non-continuous derivatives). The upper plots are gradient-sparse, where vertical and horizontal contours indicate the function is constant with respect to one of its inputs, i.e. $\partial f/\partial y = 0$ or $\partial f/\partial x = 0$. The lower plots are gradient-rich, where the curved contours indicate the function always varies with respect to any of its inputs, i.e. $\partial f/\partial y \neq 0$ and $\partial f/\partial x \neq 0$. $\partial \mathbb{B}$ nets use gradient-rich functions to ensure that error is always backpropagated to all inputs.} 
	\label{fig:gradient-rich}
\end{figure}

\subsection{Learning to negate}

We aim to learn to negate a boolean value, $x$, or simply leave it unaltered. Represent this decision by a boolean weight, $w$, where low $w$ means negate and high $w$ means do nothing. The boolean function that meets this requirement is $\neg(x \oplus w)$. However, this function is not differentiable. We therefore define the differentiable function,
	\begin{equation*}
	\begin{aligned}
	\partial \neg: [0, 1]^{2} &\to [0,1], \\
	(w, x) &\mapsto 1 - w + x (2w - 1)\text{,}
	\end{aligned}
	\end{equation*}
where $\partial \neg(w, x) \equiv \neg(x \oplus w)$ (see proposition \ref{prop:not}).

Product logics, where for example $f(x,y) = x y$ is as a soft version of $x \wedge y$, although hard-equivalent at extreme values, e.g. $f(1,1)=1$ and $f(0,1)=0$, are not hard-equivalent at intermediate values, e.g. $f(0.6, 0.6) = 0.36$. G\"{o}del-style $\operatorname{min}$ and $\operatorname{max}$ functions, although hard-equivalent over the entire soft-bit range, i.e. $\operatorname{min}(x,y) \equiv x \wedge y$ and $\operatorname{min}(x,y) \equiv x \vee y$, are gradient-sparse in the sense that their outputs are not always a function of all their inputs, e.g. $\frac{\partial}{\partial x} \operatorname{max}(x,y) = 0$ when $(x,y)=(0.1, 0.9)$. So although the composite function $\operatorname{max}(\operatorname{min}(w, x), \operatorname{min}(1-w, 1-x))$ is differentiable and $\equiv \neg(x \oplus w)$ it does not always backpropagate error to its inputs. In contrast, $\partial \neg$ is a gradient-rich function that always backpropagates error to its inputs (see figure \ref{fig:gradient-rich}). 

\subsection{Margin packing}

Say we aim to construct a differentiable analogue of $x \wedge y$. Note that $\operatorname{min}(x,y)$ essentially selects one of $x$ or $y$ as a representative soft-bit that is guaranteed hard-equivalent to $x \wedge y$. However, by selecting only one of $x$ or $y$ then $\operatorname{min}$ is also guaranteed to be gradient-sparse. We define a `margin packing' method to avoid this dilemma.

The main idea of margin packing is (i) select a representative bit that is hard-equivalent to the target discrete function, and then (ii) pack a fraction of the margin between the representative bit and the hard threshold $1/2$ with gradient-rich information. The result is an augmented bit that is a function of all inputs yet hard-equivalent to the target function.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{margin-trick.png}
	\caption{{\em Margin packing for constructing gradient-rich, hard-equivalent functions}. A representative bit, $z$, is hard-equivalent to a discrete target function but gradient-sparse (e.g. $z=\operatorname{min}(x,y) \equiv x \wedge y$). On the left $z$ is low, $z<1/2$; on the right $z$ is high, $z>1/2$. We can pack a fraction of the margin between $z$ and the hard threshold $1/2$ with additional gradient-rich information without affecting hard-equivalence. A natural choice is the mean soft-bit, $\bar{\bf x} \in [0,1]$. The grey shaded areas denote the packed margins and the final augmented bit. On the left $\approx 60\%$ of the margin is packed; on the right $\approx 90\%$.}
	\label{fig:margin-trick}
\end{figure}
% On the left, ${\bf x}=[0.9,0.23]$, $z=0.23$, $\bar{\bf x}=0.57$ and therefore $\approx 60\%$ of the margin is packed; on the right, ${\bf x}=[0.9,0.83]$,  $z=0.83$, $\bar{\bf x}=0.87$, and therefore $\approx 90\%$ of the margin is packed.

More concretely, say we have a vector of soft-bit inputs ${\bf x}$ and the $i$th element represents the target discrete function (e.g. if our target is $x \wedge y$ then ${\bf x}=[x,y]$ and $i$ is 1 if $x<y$ and $i=2$ otherwise). Now, if we pack only a fraction of the available margin, $|x_{i}-1/2|$, we will not cross the $1/2$ threshold and break the hard-equivalence of the representative bit. The average soft-bit value, $\bar{\bf x} \in [0,1]$, is just such a gradient-rich fraction. We therefore define 
\begin{equation*}
\begin{aligned}
\operatorname{margin-fraction}: [0,1]^{n} \times 1,2,\dots,n &\to [0,1],\\
({\bf x}, i) &\mapsto \bar{\bf x} \times \left|x_{i} - 1/2\right| \text{.}
\end{aligned}
\end{equation*}
The packed fraction of the margin increases or decreases with the average soft-bit value. The available margin tends to zero as the representative bit tends to the hard threshold $1/2$. At the threshold point there is no margin to pack. 

Define the augmented bit as
\begin{equation*}
\begin{aligned}
\operatorname{augmented-bit}: [0,1]^{n} \times 1,2,\dots,n &\to [0,1],\\
({\bf x}, i) &\mapsto 
\begin{cases}
1/2 + \operatorname{margin-fraction}({\bf x}, i) & \text{if } x_{i} > 1/2 \\
x_{i} + \operatorname{margin-fraction}({\bf x}, i) & \text{otherwise.}
\end{cases}
\end{aligned}
\end{equation*}
If the representative bit is high (resp. low) then the augmented bit is also high (resp. low). 
But the augmented bit has a higher (resp. lower) value than the representative bit when below (resp. above) the $1/2$ threshold. The difference depends on the size of the available margin and the mean soft-bit value. Almost everywhere, an increase (resp. decrease) of the mean soft-bit increases (resp. decreases) the value of the augmented bit (see figure \ref{fig:margin-trick}). Note that if the $i$th bit is representative (i.e. hard-equivalent to the target function) then so is the augmented bit (see proposition \ref{prop:augmented}). We now use margin packing to define gradient-rich, hard-equivalents of boolean functions.

\subsection{Differentiable $\wedge$}

We aim to construct a differentiable analogue of the boolean function $\bigwedge_{i=1}^{n} x_i$. A representative bit is $\operatorname{min}(x_{1},\dots,x_{n})$. The function
\begin{equation*}
\begin{aligned}
\partial \wedge: [0,1]^{n} &\to [0,1], \\
{\bf x} &\mapsto \operatorname{augmented-bit}({\bf x}, \operatorname{argmin}\limits_{i} x[i])
\end{aligned}
\end{equation*}
is therefore hard-equivalent to the boolean function $\bigwedge_{i=1}^{n} x_i$ (see proposition \ref{prop:and}). In the special case $n=2$ we get the piecewise function,
\begin{equation*}
\partial\!\wedge\!(x, y) =
	\begin{cases}
	1/2 + 1/2(x + y)(\operatorname{min}(x,y) - 1/2) & \text{if } \operatorname{min}(x,y) > 1/2 \\
	\operatorname{min}(x,y) + 1/2(x + y)(1/2 - \operatorname{min}(x,y)) & \text{otherwise.}
	\end{cases}
\end{equation*}
Note that $\partial \wedge$ is differentiable almost everywhere and gradient-rich (see figure \ref{fig:gradient-rich}).

\subsection{Differentiable $\vee$}

The differentiable analogue of $\vee$ is identical to $\wedge$, except the representative bit is selected by $\operatorname{max}$. The function
\begin{equation*}
\begin{aligned}
\partial\vee: [0,1]^{n} &\to [0,1], \\
{\bf x} &\mapsto \operatorname{augmented-bit}({\bf x}, \operatorname{argmax}\limits_{i} x[i])
\end{aligned}
\end{equation*}
is hard-equivalent to the boolean function $\bigvee_{i=1}^{n} x_i$ (see proposition \ref{prop:or}). Note that $\partial \vee$ is differentiable almost everywhere and gradient-rich (see figure \ref{fig:gradient-rich}).

\begin{comment}
Define 
\begin{equation*}
\begin{aligned}
\partial\!\vee\!(x, y) =
\begin{cases}
1/2 + 1/2(x + y)(\operatorname{max}(x,y) - 1/2) & \text{if } \operatorname{max}(x,y) > 1/2 \\
\operatorname{max}(x,y) + 1/2(x + y)(1/2 - \operatorname{max}(x,y)) & \text{otherwise.}
\end{cases}
\end{aligned}
\end{equation*}
\begin{comment}
\begin{equation*}
\begin{aligned}
\partial \vee: [0,1]^{2} &\to [0,1], \\
(x, y) &\mapsto 
\begin{cases}
	1/2 + 1/2(x + y)(m - 1/2) & \text{if } 2m > 1 \\
m + 1/2(x + y)(1/2 - m) & \text{otherwise,}
\end{cases}
\end{aligned}
\end{equation*}
\end{comment}

\subsection{Differentiable $\Rightarrow$}

The differentiable analogue of $\Rightarrow$ is defined in terms of $\partial\vee$. The function
\begin{equation*}
\begin{aligned}
\partial\!\Rightarrow: [0,1]^{2} &\to [0,1],\\
(x, y) &\mapsto \partial\!\vee\!(y, 1-x)\text{,}
\end{aligned}
\end{equation*}
is hard-equivalent to $x \Rightarrow y$ (proposition \ref{prop:implies}). We can define  analogues of all the basic boolean operators in a similar manner.

\subsection{Differentiable majority}

\begin{figure}[t]
	\centering
	\includegraphics[trim=0pt 0pt 0pt 0pt, clip, width=1.0\textwidth]{majority-gates.png}
	\caption{{\em Majority.}} 
	\label{fig:majority-plot}
\end{figure}

The boolean majority function is particularly important for tractable learning because it is a threshold function:
\begin{equation*}
\begin{aligned}
\operatorname{Maj}: \{0,1\}^{n} &\to \{0,1\},\\
{\bf x} &\mapsto 
	\begin{cases}
	1 &\text{if }\sum_{i=1}^n x_i \geq \ceil{\frac{n}{2}}\\
	0 &\text{otherwise.}
	\end{cases}
\text{,}
\end{aligned}
\end{equation*}
Interpret each input bit $x_{i}$ as a vote, yes or no, for a binary decision. If the majority of voters are in favour then $\operatorname{Maj}$ outputs 1. The majority function, in the context of predictive model, aggregates multiple bits of weak evidence into a hard decision. Neural network binarization transforms real-valued activation functions into boolean majority \citep{10.5555/3157382.3157557}, which demonstrates their close association. We aim to construct a differentiable analogue of $\operatorname{Maj}$.

$\operatorname{Maj}$ for $n$ bits in DNF form is a disjunction of $\binom{n}{k}$ conjunctive clauses of size $k$, where $k=\lceil n/2 \rceil$ and each clause checks if a unique combination of a majority of the $n$ bits are high; e.g. $\operatorname{Maj}(x, y, z) = (x \wedge y) \vee (x \wedge y) \vee (y \wedge z)$. Therefore, we could in principle implement a differentiable analogue of $\operatorname{Maj}$ in terms of $\partial\wedge$ and $\partial\vee$. However, the number of terms grows exponentially; e.g. $n=50$ generates over 100 trillion conjunctive clauses, which is infeasible. In circuit theory there is no known general algorithm for finding the minimal representation of $\operatorname{Maj}$ for arbitrary $n$.

Instead, we observe that if $\operatorname{sort}({\bf x})$ sorts the elements of ${\bf x}$ in ascending order then the `middle' soft-bit is representative. For example, if ${\bf x} = [0.4, 0.9, 0.2]$ then $\operatorname{sort}({\bf x}) = [0.2, 0.4, 0.9]$ and the `middle' bit $x_{2}=0.4$ is low, which is hard-equivalent to $\operatorname{Maj}(0, 1, 0) = 0$. Define the index of the `middle' bit by
\begin{equation*}
\begin{aligned}
\operatorname{majority-index}: \mathbb{Z}_{>0} \to \mathbb{Z}_{> 0}\\
n \mapsto \left\lceil \frac{n}{2}\right\rceil
\text{.}
\end{aligned}
\end{equation*}
Then the differentiable function
\begin{equation*}
\begin{aligned}
	\partial\!\operatorname{Maj}: [0,1]^{n} &\to [0,1], \\
	{\bf x} &\mapsto \operatorname{augmented-bit}(\operatorname{sort}({\bf x}), \operatorname{majority-index}({\bf x}))
\end{aligned}
\end{equation*}
is hard-equivalent to $\operatorname{Maj}({\bf x})$ (see proposition \ref{prop:majority}). Note that $\partial\!\operatorname{Maj}$ is differentiable almost everywhere and gradient-rich (see figure \ref{fig:majority-plot}).


TODO: discuss time-complexity of sort.

We also want to maximise fan-in information where necessary (for backprop error signal) but also minimize it where necessary (e.g. picking a representative bit we want to optimize). Representative bit when we want a vector-wise update; otherwise, min/max.

TODO: min/max are a degenerate case of sorting/ordering

\subsection{Differentiable integer count}

Define 
\begin{equation*}
\begin{aligned}
\operatorname{low-high}: [0,1]^{n} &\to [0,1]^{n+1},\\
{\bf x} &\mapsto \left[ \operatorname{min}(1, x_{1}), \operatorname{min}(1 - x_{1}, x_{2}), \dots, \operatorname{min}(1 - x_{n-1}, x_{n}), \operatorname{min}(x_{n}, 1) \right]\text{.}
\end{aligned}
\end{equation*}

Define
\begin{equation*}
\begin{aligned}
\partial\text{COUNT}: [0,1]^{n} &\to [0,1]^{n+1}, \\
{\bf x} &\mapsto \operatorname{low-high}(\operatorname{sort}({\bf x}))\text{,}
\end{aligned}
\end{equation*}
where ${\bf x}$ is a vector of soft-bits. $\partial${COUNT} is hard-equivalent to
\begin{equation*}
\begin{aligned}
\text{COUNT}: \{0,1\}^{n} &\to \{0,1\}^{n+1}, \\
{\bf x} &\mapsto \left[\operatorname{k-of-n}({\bf x}, 0), \operatorname{k-of-n}({\bf x}, 1), \dots, \operatorname{k-of-n}({\bf x}, n)\right]\text{,}
\end{aligned}
\end{equation*}
where
\begin{equation*}
\operatorname{k-of-n}({\bf x}, k) = \bigvee_{|S|=k} \bigwedge_{i\in S} x_i \bigwedge_{j\notin S} \neg x_j
\end{equation*}
(see proposition \ref{prop:count}).

%$f(x_1, x_2, \ldots, x_n) = \bigvee_{|S|=k} \left( \bigwedge_{i\in S} x_i \bigwedge_{j\notin S} \neg x_j \right)$
%is a logical OR that is taken over all possible subsets $S$ of size $k$ of the set ${1,2,\ldots,n}$.

TODO: count is 1-hot function
TODO: count allows us to implement boolean counting functions.

\subsection{Logical layers}

Define
\begin{equation*}
\begin{aligned}
\partial \neg \text{LAYER}: [0,1]^{n \times m} \times [0,1]^{m} &\to [0,1]^{n \times m}, \\
({\bf W}, {\bf x}) &\mapsto 
\begin{bmatrix}
\partial \neg(w_{1,1}, x_{1}) & \dots & \partial \neg(w_{1,m}, x_{m}) \\
\vdots & \ddots & \vdots \\
\partial \neg(w_{n,1}, x_{1}) & \dots & \partial \neg(w_{n,m}, x_{m})
\end{bmatrix}
\end{aligned}
\end{equation*}
where ${\bf W}$ is a matrix of weights and ${\bf x}$ is a vector of soft-bits.

%[\partial \neg({\bf W}_{1}, {\bf x}), \dots, \partial \neg({\bf W}_{n}, {\bf x})]

Define
\begin{equation*}
\begin{aligned}
\partial\!\wedge\!\text{NEURON}: [0,1]^{n} \times [0,1]^{n} &\to [0,1], \\
({\bf w}, {\bf x}) &\mapsto \min(\partial \Rightarrow(w_{1}, x_{1}), \dots, \partial \Rightarrow(w_{n}, x_{n}))\text{,}
\end{aligned}
\end{equation*}
where ${\bf w}$ is vector of weights and ${\bf x}$ is a vector of soft-bits. A single AND neuron maps $n$ soft-bit inputs to a single soft-bit value. A $\partial\!\wedge\!\text{LAYER}$ of $n$ neurons maps $m$ soft-bit inputs to $n$ soft-bit outputs.

Define
\begin{equation*}
\begin{aligned}
\partial\!\vee\!\text{NEURON}: [0,1]^{n} \times [0,1]^{n} &\to [0,1], \\
({\bf w}, {\bf x}) &\mapsto \max(\partial \wedge(w_{1}, x_{1}), \dots, \partial \wedge(w_{n}, x_{n}))\text{.}
\end{aligned}
\end{equation*}
A single $\vee$ neuron maps $n$ soft-bit inputs to a single soft-bit value. A $\partial\!\vee\!\text{LAYER}$ of $n$ neurons maps $m$ soft-bit inputs to $n$ soft-bit outputs.

\subsection{Architectures}

todo: architectures

todo: multi-label classification

Define
\begin{equation*}
\begin{aligned}
\partial\text{HARDEN}: [0,1]^{n} &\to [0,1]^{n}, \\
{\bf x} &\mapsto \operatorname{harden}({\bf x})\text{.}
\end{aligned}
\end{equation*}
A $\partial\text{HARDEN}$ layer maps $m$ soft-bit inputs to $n$ hard-bit outputs. The $\operatorname{harden}$ function is not differentiable and therefore we use the straight-through estimator \citep{DBLP:journals/corr/BengioLC13} during backpropagation.


\section{Experiments}

\subsection{Hardening}

\subsection{Binary Iris}

\subsection{Noisy XOR}

The noisy XOR dataset \citep{noisy-xor-dataset} is an adversarial parity problem with noisy non-informative features. The dataset consists of 10K examples with 12 boolean inputs and a target label (where 0 = odd and 1 = even) that is a XOR function of 2 inputs. The remaining 10 inputs are entirely random. We train on 50\% of the data where, additionally, 40\% of the labels are inverted.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{noisy-xor-architecture.png}
	\caption{{\em A $\partial\mathbb{B}$ net for the noisy-xor problem}. The net concatenates the soft-bit input, ${\bf x}$ (length 12), with its negation, ${\bf 1 - x}$, and supplies the resulting vector (length 24) to a $\partial\!\wedge\!\text{LAYER}$ (width 32), $\partial\!\vee\!\text{LAYER}$ (width 32),  $\partial \neg \text{LAYER}$ (width 16), and a final $\partial\!\operatorname{Maj}$ to produce a single soft-bit $y \in [0,1]$ (to predict odd parity) and its negation $1-y$ (to predict even parity). The net's weights, once hardened, consume $288$ bytes.}
	\label{fig:noisy-xor-architecture}
\end{figure}

We initialized the network described in figure \ref{fig:noisy-xor-architecture} using the XX policy and then trained for 2000 epochs with the RAdam optimizer \citep{Liu2020On} and softmax cross-entropy loss. We measure the accuracy of the final net on the test data (to avoid handpicking the best configuration). Table \ref{tab:noisy-xor-results} compares the $\partial\mathbb{B}$ net against other classifiers \citep{granmo18} and reports the mean accuracy with 95\% confidence intervals obtained over 100 replications of the experiment with different random seeds. 

\begin{table}[h]
	\centering
	\begin{tabular}{llllll}
		\cline{2-6}
		\multicolumn{1}{c}{}                       & \multicolumn{5}{c}{\textbf{accuracy}}                                                                                                                                                            \\ \cline{2-6} 
		\multicolumn{1}{l|}{}                      & \multicolumn{1}{l|}{mean}                  & \multicolumn{1}{l|}{5 \%ile}       & \multicolumn{1}{l|}{95 \%ile}       & \multicolumn{1}{l|}{min}           & \multicolumn{1}{l|}{max}            \\ \hline
		\multicolumn{1}{|l|}{Tsetlin}              & \multicolumn{1}{l|}{99.3 +/- 0.3}          & \multicolumn{1}{l|}{95.9}          & \multicolumn{1}{l|}{100.0}          & \multicolumn{1}{l|}{91.6}          & \multicolumn{1}{l|}{100.0}          \\ \hline
		\multicolumn{1}{|l|}{$\partial\mathbb{B}$} & \multicolumn{1}{l|}{\textbf{97.9 +/- 0.2}} & \multicolumn{1}{l|}{\textbf{95.4}} & \multicolumn{1}{l|}{\textbf{100.0}} & \multicolumn{1}{l|}{\textbf{93.6}} & \multicolumn{1}{l|}{\textbf{100.0}} \\ \hline
		\multicolumn{1}{|l|}{neural network}       & \multicolumn{1}{l|}{95.4 +/- 0.5}          & \multicolumn{1}{l|}{90.1}          & \multicolumn{1}{l|}{98.6}           & \multicolumn{1}{l|}{88.2}          & \multicolumn{1}{l|}{99.9}           \\ \hline
		\multicolumn{1}{|l|}{SVM}                  & \multicolumn{1}{l|}{58.0 +/- 0.3}          & \multicolumn{1}{l|}{56.4}          & \multicolumn{1}{l|}{59.2}           & \multicolumn{1}{l|}{55.4}          & \multicolumn{1}{l|}{66.5}           \\ \hline
		\multicolumn{1}{|l|}{naive Bayes}          & \multicolumn{1}{l|}{49.8 +/- 0.2}          & \multicolumn{1}{l|}{48.3}          & \multicolumn{1}{l|}{51.0}           & \multicolumn{1}{l|}{41.3}          & \multicolumn{1}{l|}{52.7}           \\ \hline
		\multicolumn{1}{|l|}{logistic regression}  & \multicolumn{1}{l|}{49.8 +/- 0.3}          & \multicolumn{1}{l|}{47.8}          & \multicolumn{1}{l|}{51.1}           & \multicolumn{1}{l|}{41.1}          & \multicolumn{1}{l|}{53.1}           \\ \hline
	\end{tabular}
	\caption{{\em Ranked noisy-XOR results}}
	\label{tab:noisy-xor-results}
\end{table}

The high noise causes logistic regression and naive Bayes to randomly guess. The SVM hardly performs better. In constrast, the multilayer neural network, Tsetlin machine \citep{granmo18}, and  $\partial\mathbb{B}$ net all successfully learn the underlying XOR signal. The Tsetlin machine performs best on this problem, with the $\partial\mathbb{B}$ net second.

todo: can Tsetlin machines be chained in differentiable architectures?

\subsection{MNIST}

\section{Conclusion}

TODO: we can write boolean functions, soften them, and place them within differentiable neural networks.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliographystyle{iclr2021_conference}
\bibliography{db}

\appendix

\section*{Appendix}

\section{Proofs}

\begin{prop}\label{prop:not}
	$\partial \neg(x,y) \equiv \neg (x \oplus y)$.
\begin{proof}
	Table \ref{not-table} is the truth table of the boolean function $\neg (x \oplus w)$, where $h(x) = \operatorname{harden}(x)$.
	\begin{table}
		\begin{center}
			\begin{tabular}{cccccc}
				\multicolumn{1}{c}{$x$}  &\multicolumn{1}{c}{$y$}  &\multicolumn{1}{c}{$h(x)$}  &\multicolumn{1}{c}{$h(y)$} &\multicolumn{1}{c}{$\partial \neg(h(x), h(y))$} &\multicolumn{1}{c}{$h(\partial \neg(h(x), h(y)))$}
				\\ \hline \\
				$\left[0, \frac{1}{2}\right]$ & $\left[0, \frac{1}{2}\right]$ & 0 & 0 & 1 & 1\\[0.1cm]
				$\left(\frac{1}{2}, 1\right]$ & $\left[0, \frac{1}{2}\right]$ &1 & 0 & 0 & 0\\[0.1cm]
				$\left[0, \frac{1}{2}\right]$ & $\left(\frac{1}{2}, 1\right]$ &0 & 1 & 0 & 0\\[0.1cm]
				$\left(\frac{1}{2}, 1\right]$ & $\left(\frac{1}{2}, 1\right]$ &1 & 1 & 1 & 1\\[0.1cm]
			\end{tabular}
		\end{center}
		\caption{$\partial \neg(x,y) \equiv \neg (y \oplus x)$.}\label{not-table}
	\end{table}
\end{proof}
\end{prop}

\begin{prop}\label{prop:augmented}
	If a representative bit is hard-equivalent to a target function then so is the augmented bit.
	\begin{proof}
		todo
	\end{proof}
\end{prop}


\begin{prop}\label{prop:and}
	$\partial\!\wedge\!(x,y) \equiv x \wedge y$.
\begin{proof}
	Table \ref{and-table} is the truth table of the boolean function $x \wedge y$, where $h(x) = \operatorname{harden}(x)$..
	\begin{table}
		\begin{center}
			\begin{tabular}{cccccc}
				\multicolumn{1}{c}{$x$}  &\multicolumn{1}{c}{$y$}  &\multicolumn{1}{c}{$h(x)$}  &\multicolumn{1}{c}{$h(y)$} &\multicolumn{1}{c}{$\partial \wedge(h(x), h(y))$} &\multicolumn{1}{c}{$h(\partial \wedge(h(x), h(y)))$}
				\\ \hline \\
				$\left[0, \frac{1}{2}\right]$ & $\left[0, \frac{1}{2}\right]$ & 0 & 0 & 0 & 0\\[0.1cm]
				$\left(\frac{1}{2}, 1\right]$ & $\left[0, \frac{1}{2}\right]$ &1 & 0 & $\frac{1}{4}$ & 0\\[0.1cm]
				$\left[0, \frac{1}{2}\right]$ & $\left(\frac{1}{2}, 1\right]$ &0 & 1 & $\frac{1}{4}$ & 0\\[0.1cm]
				$\left(\frac{1}{2}, 1\right]$ & $\left(\frac{1}{2}, 1\right]$ &1 & 1 & 1 & 1\\[0.1cm]
			\end{tabular}
		\end{center}
		\caption{$\partial \wedge(x,y) \equiv x \wedge y$.}\label{and-table}
	\end{table}			
\end{proof}
\end{prop}

\begin{prop}\label{prop:or}
	$\partial\!\vee\!(x,y) \equiv x \vee y$.
\begin{proof}
	Table \ref{or-table} is the truth table of the boolean function $x \vee y$, where $h(x) = \operatorname{harden}(x)$..
	\begin{table}
	\begin{center}
		\begin{tabular}{cccccc}
			\multicolumn{1}{c}{$x$}  &\multicolumn{1}{c}{$y$}  &\multicolumn{1}{c}{$h(x)$}  &\multicolumn{1}{c}{$h(y)$} &\multicolumn{1}{c}{$\partial \vee(h(x), h(y))$} &\multicolumn{1}{c}{$h(\partial \vee(h(x), h(y)))$}
			\\ \hline \\
			$\left[0, \frac{1}{2}\right]$ & $\left[0, \frac{1}{2}\right]$ & 0 & 0 & 0 & 0\\[0.1cm]
			$\left(\frac{1}{2}, 1\right]$ & $\left[0, \frac{1}{2}\right]$ &1 & 0 & $\frac{3}{4}$ & 1\\[0.1cm]
			$\left[0, \frac{1}{2}\right]$ & $\left(\frac{1}{2}, 1\right]$ &0 & 1 & $\frac{3}{4}$ & 1\\[0.1cm]
			$\left(\frac{1}{2}, 1\right]$ & $\left(\frac{1}{2}, 1\right]$ &1 & 1 & 1 & 1\\[0.1cm]
		\end{tabular}
	\end{center}
	\caption{$\partial \vee(x,y) \equiv x \vee y$.}\label{or-table}
	\end{table}			
\end{proof}
\end{prop}

\begin{prop}\label{prop:implies}
	$\partial\!\Rightarrow\!(x,y) \equiv x \Rightarrow y$.
\begin{proof}
	Table \ref{implies-table} is the truth table of the boolean function $x \Rightarrow y$, where $h(x) = \operatorname{harden}(x)$..
	\begin{table}
	\begin{center}
		\begin{tabular}{cccccc}
			\multicolumn{1}{c}{$x$}  &\multicolumn{1}{c}{$y$}  &\multicolumn{1}{c}{$h(x)$}  &\multicolumn{1}{c}{$h(y)$} &\multicolumn{1}{c}{$\partial \Rightarrow(h(x), h(y))$} &\multicolumn{1}{c}{$h(\partial \Rightarrow(h(x), h(y)))$}
			\\ \hline \\
			$\left[0, \frac{1}{2}\right]$ & $\left[0, \frac{1}{2}\right]$ & 0 & 0 & $\frac{3}{4}$ & 1\\[0.1cm]
			$\left(\frac{1}{2}, 1\right]$ & $\left[0, \frac{1}{2}\right]$ &1 & 0 & 0 & 0\\[0.1cm]
			$\left[0, \frac{1}{2}\right]$ & $\left(\frac{1}{2}, 1\right]$ &0 & 1 & 1 & 1\\[0.1cm]
			$\left(\frac{1}{2}, 1\right]$ & $\left(\frac{1}{2}, 1\right]$ &1 & 1 & $\frac{3}{4}$ & 1\\[0.1cm]
		\end{tabular}
	\end{center}
	\caption{$\partial \Rightarrow(x,y) \equiv x \Rightarrow y$.}\label{implies-table}
	\end{table}			
\end{proof}
\end{prop}

\begin{lemma}
$\operatorname{majority-bit}$ is hard-equivalent to the boolean majority function.
\begin{proof}
	The boolean majority function,
\begin{equation*}
\text{MAJ}(x_{1}, \dots, x_{n}) = 
\begin{cases}
1 & \text{if } \sum_{i=1}^{n} x_{i} \geq n/2 \\
0 & \text{otherwise.}
\end{cases}
\end{equation*}
\end{proof}
\end{lemma}

\begin{prop}\label{prop:majority}
	$\partial${MAJORITY} is hard-equivalent to the boolean majority function.
\begin{proof}
	todo
\end{proof}
\end{prop}

\begin{prop}\label{prop:count}
	$\partial${COUNT} is hard-equivalent to COUNT.
	\begin{proof}
		todo
	\end{proof}
\end{prop}


\end{document}

\begin{comment}
Define
\begin{equation*}
\begin{aligned}
\partial\text{AND-LAYER}: [0,1]^{n \times m} \times [0,1]^{m} &\to [0,1]^{n}, \\
({\bf W}, {\bf x}) &\mapsto [\partial\wedge\text{NEURON}({\bf W}_{1}, {\bf x}), \dots, \partial\wedge\text{NEURON}({\bf W}_{n}, {\bf x})]
\end{aligned}
\end{equation*}
\end{comment}
